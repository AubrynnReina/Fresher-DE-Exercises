{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Management in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark memory management is divided into two types:\n",
    "\n",
    "- Static Memory Manager (*Deprecated* since Spark 1.6).\n",
    "- Unified Memory Manager (*default*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Memory Manager (UMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/Cd_Unified_Memory_Manager_5GB.jpg)\n",
    "\n",
    "[Source](https://community.cloudera.com/t5/Community-Articles/Spark-Memory-Management/ta-p/317794)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of UMM:\n",
    "- The boundary between storage memory and execution memory is not static, and in cases of memory pressure, the boundary would be moved, i.e., one region would grow by borrowing space from another one.\n",
    "- When the application has no cache and is propagating, execution uses all the memory to avoid unnecessary disk overflow.\n",
    "- When the application has a cache, it will reserve the minimum storage memory so that the data block is not affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JVM has two types of memory:\n",
    "\n",
    "- On-Heap Memory.\n",
    "- Off-Heap Memory.\n",
    "\n",
    "There is one more segment of memory that is accessed by Spark, i.e., external process memory, mainly used for PySpark and SparkR applications, resides outside the JVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Heap Memory (*default*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the on-heap memory is configured by the --executor-memory or spark.executor.memory parameter when the Spark application starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main configurations to control executor memory allocation:\n",
    "\n",
    "| Parameter | Description |\n",
    "| -- | -- |\n",
    "| spark.memory.fraction (default 0.6) | Fraction of the heap space used for execution and storage. The lower this is, the more frequently spills and cached data evictions occur. The purpose of this configuration is to set aside memory for internal metadata, user data structures, and imprecise size estimation in the case of sparse, unusually large records. |\n",
    "| spark.memory.storageFraction (default 0.5) | The size of the storage region within the space set aside by spark.memory.fraction. Cached data may only be evicted if total storage exceeds this region. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark supports three memory regions:\n",
    "\n",
    "- Reserved Memory (**hard-coded** 300MB):\n",
    "    - Reserved for the system and is used to store Spark's internal objects.\n",
    "    - If executor memory is less than 1.5 times the reserved memory (450MB), Spark will raise an error.\n",
    "\n",
    "    ![image.png](./images/low_exe_memory_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- User Memory: *(Java Heap — Reserved Memory) * (1.0 — spark.memory.fraction)*\n",
    "    - Used to store user-defined data structures, Spark internal metadata, any UDFs created by the user, and the data needed for RDD conversion operations, such as RDD dependency information, etc.\n",
    "    - Is 40% of (Java Heap - Reserved Memory) by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark Memory: *(Java Heap — Reserved Memory) * spark.memory.fraction*\n",
    "    - Is 60% of (Java Heap - Reserved Memory) by default.\n",
    "    - Execution: \n",
    "        - Used for shuffles, joins, sorts, and aggregations.\n",
    "        - Supports spilling on disk if memory's not enough.\n",
    "        - Can't be forcefully evicted by other threads.\n",
    "        - Evicted immediately after each operation.\n",
    "    - Storage: \n",
    "        - Used to cache partitions of data.\n",
    "        - Can be evicted then:\n",
    "            - Written to disk if persistence level is MEMORY_AND_DISK.\n",
    "            - Recomputed when needed if persistence level is MEMORY_ONLY.\n",
    "\n",
    "    ***Note**: Execution Memory > Storage Memory*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Heap Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Means allocating memory objects (serialized to byte array) to memory outside the heap of the Java virtual machine (JVM).\n",
    "- Managed by the OS.\n",
    "- Stored outside the process heap in native memory &rarr; not processed by the *garbage collector*\n",
    "- Slower than On-Heap, faster than disk.\n",
    "- User has to maually deal with managing the allocated memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two main configurations to set Off-Heap Memory:\n",
    "\n",
    "| Parameter | Description |\n",
    "| -- | -- |\n",
    "| spark.memory.offHeap.enabled (default false) | The option to use off-heap memory for certain operations |\n",
    "| spark.memory.offHeap.size (default 0) | The total amount of memory in bytes for off-heap allocation. It has no impact on heap memory usage, so make sure not to exceed your executor’s total limits. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Reduce memory usage, reduce frequent GC, and improve program performance.\n",
    "- When an executor is killed, all cached data for that executor would be gone but with off-heap memory, the data would persist.\n",
    "\n",
    "Disadvantages: (?)\n",
    "- Using OFF_HEAP does not back up data, nor can it guarantee high data availability and data loss requires recalculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the storage memory in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Heap Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "spark-shell \\\n",
    "    --executor-memory 4g \\\n",
    "    --driver-memory 4g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/storage_mem_on_heap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java Heap Memory: 4096 MiB\n",
      "Reserved Memory: 300 MiB\n",
      "Usable Memory: 3796 MiB\n",
      "User Memory: 1518.4 MiB\n",
      "--> Spark Memory: 2277.6 MiB = 2.22 GiB\n",
      "Spark Storage Memory: 1138.8 MiB\n",
      "Spark Execution Memory: 1138.8 MiB\n"
     ]
    }
   ],
   "source": [
    "java_heap_mem = 4 * 1024\n",
    "reserved_mem = 300\n",
    "usable_mem = java_heap_mem - reserved_mem\n",
    "user_mem = round(0.4 * usable_mem, 2)\n",
    "spark_mem = usable_mem - user_mem\n",
    "spark_storage_mem = round(spark_mem / 2, 2)\n",
    "spark_execution_mem = round(spark_mem / 2, 2)\n",
    "\n",
    "print(f'Java Heap Memory: {java_heap_mem} MiB')\n",
    "print(f'Reserved Memory: {reserved_mem} MiB')\n",
    "print(f'Usable Memory: {usable_mem} MiB')\n",
    "print(f'User Memory: {user_mem} MiB')\n",
    "print(f'--> Spark Memory: {spark_mem} MiB = {round(spark_mem / 1024, 2)} GiB')\n",
    "print(f'Spark Storage Memory: {spark_storage_mem} MiB')\n",
    "print(f'Spark Execution Memory: {spark_execution_mem} MiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more accuracy, we can get the max memory used by spark through the termial:\n",
    "\n",
    "```scala\n",
    "val maxMemory = Runtime.getRuntime.maxMemory()\n",
    "```\n",
    "\n",
    "![image.png](./images/scala_get_max_mem.png)\n",
    "\n",
    "4294967296 B = 4 GiB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-Heap Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```console\n",
    "spark-shell \\\n",
    "    --driver-memory 4g \\\n",
    "    --executor-memory 4g \\\n",
    "    --conf spark.memory.offHeap.enabled=true \\\n",
    "    --conf spark.memory.offHeap.size=4g\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/storage_mem_off_heap.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Off Heap Memory: 4096 MiB\n",
      "--> Total Spark Memory: 6373.6 MiB = 6.22 GiB\n"
     ]
    }
   ],
   "source": [
    "off_heap_mem = 4 * 1024\n",
    "total_spark_mem = spark_mem + off_heap_mem\n",
    "\n",
    "print(f'Off Heap Memory: {java_heap_mem} MiB')\n",
    "print(f'--> Total Spark Memory: {total_spark_mem} MiB = {round(total_spark_mem / 1024, 2)} GiB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Memory Management with YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](./images/executor_memory_yarn.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the same, with an overhead memory of 10% of `--executor-memory`, minimum of 384 MB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
