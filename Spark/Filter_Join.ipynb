{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up experiment: which is faster, filter &rarr; join or join &rarr; filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "list_len = 200000\n",
    "key_list = [random.randrange(0, 100, 1) for _ in range(list_len)]\n",
    "value_list = random.sample(range(list_len), list_len)\n",
    "\n",
    "with open('text_files/filter_join_test/data_1.txt', 'w') as f:\n",
    "    for i in range(len(key_list)):\n",
    "        f.write(f'({key_list[i]}, {value_list[i]})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "list_len = 200000\n",
    "key_list = [random.randrange(0, 100, 1) for _ in range(list_len)]\n",
    "value_list = random.sample(range(list_len), list_len)\n",
    "\n",
    "with open('text_files/filter_join_test/data_2.txt', 'w') as f:\n",
    "    for i in range(len(key_list)):\n",
    "        f.write(f'({key_list[i]}, {value_list[i]})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tuple(str_data):\n",
    "    key, value = str_data[1:-1].split(', ')\n",
    "    return (key, int(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join &rarr; Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark_conf = SparkConf().setMaster('local[*]').setAppName('Filter_Join experiment')\n",
    "spark_context = SparkContext().getOrCreate(spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd_1 = spark_context.textFile('text_files/filter_join_test/data_1.txt').map(convert_to_tuple)\n",
    "key_value_rdd_2 = spark_context.textFile('text_files/filter_join_test/data_2.txt').map(convert_to_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd_1 = key_value_rdd_1.join(key_value_rdd_2) \\\n",
    "                              .filter(lambda x: int(x[0]) >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd_1 = result_rdd_1.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319531914"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_1.count() # 2m 43.5s || 2m 39s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('86', (147676, 40112)),\n",
       " ('86', (147676, 75016)),\n",
       " ('86', (147676, 89082)),\n",
       " ('86', (147676, 39070)),\n",
       " ('86', (147676, 147901)),\n",
       " ('86', (147676, 141902)),\n",
       " ('86', (147676, 60019)),\n",
       " ('86', (147676, 31920)),\n",
       " ('86', (147676, 52481)),\n",
       " ('86', (147676, 13541))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter &rarr; Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark_conf = SparkConf().setMaster('local[*]').setAppName('Filter_Join experiment')\n",
    "spark_context = SparkContext().getOrCreate(spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd_1 = spark_context.textFile('text_files/filter_join_test/data_1.txt').map(convert_to_tuple)\n",
    "key_value_rdd_2 = spark_context.textFile('text_files/filter_join_test/data_2.txt').map(convert_to_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd_2 = key_value_rdd_1.filter(lambda x: int(x[0]) >= 20) \\\n",
    "                              .join(key_value_rdd_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd_2 = result_rdd_2.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "319531914"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_2.count() # 47s || 46.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('86', (147676, 40112)),\n",
       " ('86', (147676, 75016)),\n",
       " ('86', (147676, 89082)),\n",
       " ('86', (147676, 39070)),\n",
       " ('86', (147676, 147901)),\n",
       " ('86', (147676, 141902)),\n",
       " ('86', (147676, 60019)),\n",
       " ('86', (147676, 31920)),\n",
       " ('86', (147676, 52481)),\n",
       " ('86', (147676, 13541))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
