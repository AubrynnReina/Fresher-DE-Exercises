{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up experiment: which is faster, filter &rarr; join or join &rarr; filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "list_len = 300000\n",
    "key_list = [random.randrange(0, 100, 1) for _ in range(list_len)]\n",
    "value_list = random.sample(range(list_len), list_len)\n",
    "\n",
    "with open('text_files/filter_join_test/data_1.txt', 'w') as f:\n",
    "    for i in range(len(key_list)):\n",
    "        f.write(f'({key_list[i]}, {value_list[i]})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "\n",
    "list_len = 300000\n",
    "key_list = [random.randrange(0, 100, 1) for _ in range(list_len)]\n",
    "value_list = random.sample(range(list_len), list_len)\n",
    "\n",
    "with open('text_files/filter_join_test/data_2.txt', 'w') as f:\n",
    "    for i in range(len(key_list)):\n",
    "        f.write(f'({key_list[i]}, {value_list[i]})\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tuple(str_data):\n",
    "    key, value = str_data[1:-1].split(', ')\n",
    "    return (key, int(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join &rarr; Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark_conf = SparkConf().setMaster('local[*]').setAppName('Filter_Join experiment')\n",
    "spark_context = SparkContext().getOrCreate(spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd_1 = spark_context.textFile('text_files/filter_join_test/data_1.txt').map(convert_to_tuple)\n",
    "key_value_rdd_2 = spark_context.textFile('text_files/filter_join_test/data_2.txt').map(convert_to_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd_1 = key_value_rdd_1.join(key_value_rdd_2) \\\n",
    "                              .filter(lambda x: int(x[0]) >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_1.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718924270"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_1.count() # 2m 43.5s || 2m 39s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('86', (137034, 228832)),\n",
       " ('86', (137034, 67487)),\n",
       " ('86', (137034, 155298)),\n",
       " ('86', (137034, 218202)),\n",
       " ('86', (137034, 61189)),\n",
       " ('86', (137034, 271712)),\n",
       " ('86', (137034, 164849)),\n",
       " ('86', (137034, 122707)),\n",
       " ('86', (137034, 84715)),\n",
       " ('86', (137034, 41733))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter &rarr; Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark_conf = SparkConf().setMaster('local[*]').setAppName('Filter_Join experiment')\n",
    "spark_context = SparkContext().getOrCreate(spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_rdd_1 = spark_context.textFile('text_files/filter_join_test/data_1.txt').map(convert_to_tuple)\n",
    "key_value_rdd_2 = spark_context.textFile('text_files/filter_join_test/data_2.txt').map(convert_to_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rdd_2 = key_value_rdd_1.filter(lambda x: int(x[0]) >= 20) \\\n",
    "                              .join(key_value_rdd_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[15] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_2.repartition(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718924270"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_2.count() # 47s || 46.8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('86', (137034, 228832)),\n",
       " ('86', (137034, 67487)),\n",
       " ('86', (137034, 155298)),\n",
       " ('86', (137034, 218202)),\n",
       " ('86', (137034, 61189)),\n",
       " ('86', (137034, 271712)),\n",
       " ('86', (137034, 164849)),\n",
       " ('86', (137034, 122707)),\n",
       " ('86', (137034, 84715)),\n",
       " ('86', (137034, 41733))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_rdd_2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
